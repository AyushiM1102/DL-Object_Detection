{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_RCNN_2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amcheyre-nw/DL_Object_detection/blob/main/Faster_RCNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eHDXrkgp5ZT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba8506e-486e-4637-f55e-25900e9a0646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 100346, done.\u001b[K\n",
            "remote: Counting objects: 100% (35477/35477), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2782/2782), done.\u001b[K\n",
            "remote: Total 100346 (delta 32800), reused 34905 (delta 32510), pack-reused 64869\u001b[K\n",
            "Receiving objects: 100% (100346/100346), 198.82 MiB | 34.48 MiB/s, done.\n",
            "Resolving deltas: 100% (85872/85872), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "# Download TorchVision repo to use some files from references/detection\n",
        "!pip install pycocotools --quiet\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!git checkout v0.3.0\n",
        "\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "! pip install albumentations==0.4.6"
      ],
      "metadata": {
        "id": "2i3qeD4VEGRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff625d82-f5f0-4213-b3ac-299a9548d77d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.7)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=220fed1390fbc1fa2428b06f2c79d06e672db4729c29d7f8c00801b67802da07\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "# xml library for parsing xml files\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# these are the helper libraries imported.\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations (not working this)\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "metadata": {
        "id": "eDX6htmdAMRy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BBD100K Dataset"
      ],
      "metadata": {
        "id": "tlQQdpnwDLht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import database\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTJ5-pheTZEs",
        "outputId": "01b058f7-ece4-483a-a425-2c4fe1841a1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = '/content/drive/MyDrive/BDD100K/sample_train'\n",
        "test_dir = '/content/drive/MyDrive/BDD100K/sample_val'"
      ],
      "metadata": {
        "id": "6GDnJgq91oii"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the json files\n",
        "train_path = '/content/drive/MyDrive/BDD100K/labels_FRCNN/bdd100k_labels_images_train.json'\n",
        "test_path = '/content/drive/MyDrive/BDD100K/labels_FRCNN/bdd100k_labels_images_val.json'\n",
        "\n",
        "json_train = json.load(open(train_path))\n",
        "json_test = json.load(open(test_path))\n",
        "\n",
        "# We create dictonaries were the keys are the name of the image and values are the json content\n",
        "dict_train = {}\n",
        "for j in json_train:\n",
        "  dict_train[j['name']] = j\n",
        "\n",
        "dict_test = {}\n",
        "for j in json_test:\n",
        "  dict_test[j['name']] = j"
      ],
      "metadata": {
        "id": "qujxzV7ykV3S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfDrivingCarDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, files_dir, width, height, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.files_dir = files_dir\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        \n",
        "        # sorting the images for consistency\n",
        "        # To get images, the extension of the filename is checked to be jpg\n",
        "        image_list = []\n",
        "        for image in sorted(os.listdir(self.files_dir)):\n",
        "          if image[-4:]=='.jpg' and image in dict_train.keys() or image in dict_test.keys():\n",
        "            image_list.append(image)\n",
        "        \n",
        "        self.imgs = image_list\n",
        "        #\n",
        "        self.imgs = [image for image in sorted(os.listdir(self.files_dir)) if image[-4:]=='.jpg']\n",
        "        \n",
        "        # classes: 0 index is reserved for background\n",
        "        # classes bike -> bicycle, motor -> motorcycle, pedestrian -> person (check in COCO)\n",
        "        self.classes = [_,'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle',\n",
        "                        'traffic light', 'traffic sign']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cv2.setNumThreads(0)\n",
        "        img_name = self.imgs[idx]\n",
        "        image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        scale_percent = 20 # percent of original size\n",
        "        self.width = int(img.shape[1] * scale_percent / 100)\n",
        "        self.height = int(img.shape[0] * scale_percent / 100)\n",
        "        dim = (self.width, self.height)\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "\n",
        "        img_res = cv2.resize(img_rgb, dim, cv2.INTER_AREA)\n",
        "        img_res /= 255.0\n",
        "        \n",
        "        # annotation file\n",
        "\n",
        "        if self.files_dir == files_dir:\n",
        "          data_dict = dict_train\n",
        "        else:\n",
        "          data_dict = dict_test\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        images = []\n",
        "        \n",
        "        # cv2 image gives size as height x width\n",
        "        wt = img.shape[1]\n",
        "        ht = img.shape[0]\n",
        "        \n",
        "        # get information from the image\n",
        "        for i in range(len(data_dict[img_name]['labels'])):\n",
        "          label = data_dict[img_name]['labels'][i]['category']\n",
        "          if label in self.classes:\n",
        "            labels.append(self.classes.index(label))\n",
        "            \n",
        "            # bounding box\n",
        "            xmin = data_dict[img_name]['labels'][i]['box2d']['x1']\n",
        "            xmax = data_dict[img_name]['labels'][i]['box2d']['x2']\n",
        "            \n",
        "            ymin = data_dict[img_name]['labels'][i]['box2d']['y1']\n",
        "            ymax = data_dict[img_name]['labels'][i]['box2d']['y2']\n",
        "                  \n",
        "            xmin_corr = (xmin/wt)*self.width\n",
        "            xmax_corr = (xmax/wt)*self.width\n",
        "            ymin_corr = (ymin/ht)*self.height\n",
        "            ymax_corr = (ymax/ht)*self.height\n",
        "            \n",
        "            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "        \n",
        "        # convert boxes into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "        # getting the areas of the boxes\n",
        "        print(boxes)\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        # image_id\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            \n",
        "            sample = self.transforms(image = img_res,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "            \n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "        return img_res, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = SelfDrivingCarDataset(files_dir, 1280, 720)\n",
        "print('length of dataset = ', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[150]\n",
        "print(img.shape, '\\n', target)"
      ],
      "metadata": {
        "id": "SM-o5OTGBfC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33eb513d-83b5-4f91-9df1-44390854b866"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset =  11448 \n",
            "\n",
            "tensor([[ 45.1571,  32.8324,  55.7353,  43.2111],\n",
            "        [ 45.1571,  42.8119,  56.1345,  45.6061],\n",
            "        [ 45.5562,  46.0053,  56.5337,  55.7852],\n",
            "        [ 62.5213,  41.4148,  64.5172,  43.2111],\n",
            "        [ 62.9205,  39.4189,  64.9164,  41.0156],\n",
            "        [102.0400,  44.4086, 105.4330,  46.4045]])\n",
            "(144, 256, 3) \n",
            " {'boxes': tensor([[ 45.1571,  32.8324,  55.7353,  43.2111],\n",
            "        [ 45.1571,  42.8119,  56.1345,  45.6061],\n",
            "        [ 45.5562,  46.0053,  56.5337,  55.7852],\n",
            "        [ 62.5213,  41.4148,  64.5172,  43.2111],\n",
            "        [ 62.9205,  39.4189,  64.9164,  41.0156],\n",
            "        [102.0400,  44.4086, 105.4330,  46.4045]]), 'labels': tensor([10, 10, 10,  9,  9,  3]), 'area': tensor([109.7877,  30.6736, 107.3577,   3.5852,   3.1869,   6.7721]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'image_id': tensor([150])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of bounding boxes"
      ],
      "metadata": {
        "id": "iV86SfIPDSFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_img_bbox(img, target):\n",
        "    # plot the image and bboxes\n",
        "    # Bounding boxes are defined as follows: x-min y-min width height\n",
        "    fig, a = plt.subplots(1,1)\n",
        "    fig.set_size_inches(5,5)\n",
        "    a.imshow(img)\n",
        "\n",
        "    for box in (target['boxes']):\n",
        "      \n",
        "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 2,\n",
        "                                 edgecolor = 'r',\n",
        "                                 facecolor = 'none')\n",
        "\n",
        "        # Draw the bounding box on top of the image\n",
        "        a.add_patch(rect)\n",
        "    plt.show()\n",
        "    \n",
        "# plotting the image with bboxes. Feel free to change the index\n",
        "img, target = dataset[200]\n",
        "plot_img_bbox(img, target)"
      ],
      "metadata": {
        "id": "IVQOnu42Btzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pre-trained model Fast RCNN"
      ],
      "metadata": {
        "id": "EPuFaJVbDXFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained model\n",
        "def get_object_detection_model(num_classes):\n",
        "\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, trainable_backbone_layers=3)\n",
        "\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False \n",
        "    \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_UG78sLW5tTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "-eS-ROSFDbxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send train=True fro training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "    \n",
        "    if train:\n",
        "        return A.Compose([\n",
        "                            A.HorizontalFlip(0.5),\n",
        "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "                            ToTensorV2(p=1.0) \n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "    else:\n",
        "        return A.Compose([\n",
        "                            ToTensorV2(p=1.0)\n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "ddZQojpBB9fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing dataset"
      ],
      "metadata": {
        "id": "tr9wmyiOCLLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = SelfDrivingCarDataset(files_dir, 1280, 720, transforms= get_transform(train=True))\n",
        "dataset_test = SelfDrivingCarDataset(files_dir, 1280, 720, transforms= get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=16, shuffle=True, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=16, shuffle=False, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n"
      ],
      "metadata": {
        "id": "MMlxqxjLCP2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "zSQRx_HpChAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to train on gpu if selected.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 11\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "metadata": {
        "id": "c3aF5XBpCi8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training for 2 epochs\n",
        "num_epochs = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "metadata": {
        "id": "1dHLDYXHCUBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding predictions"
      ],
      "metadata": {
        "id": "MJgX7x0ZCvLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the function takes the original prediction and the iou threshold.\n",
        "\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    \n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    \n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    \n",
        "    return final_prediction\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "def torch_to_pil(img):\n",
        "    return torchtrans.ToPILImage()(img).convert('RGB')"
      ],
      "metadata": {
        "id": "Uz7UQblPCxMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ],
      "metadata": {
        "id": "sh8YlJeKC1G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick one image from the test set\n",
        "img, target = dataset_test[10]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "print('predicted #boxes: ', len(prediction['labels']))\n",
        "print('real #boxes: ', len(target['labels']))"
      ],
      "metadata": {
        "id": "g3vJcpRKC2U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('EXPECTED OUTPUT')\n",
        "plot_img_bbox(torch_to_pil(img), target)"
      ],
      "metadata": {
        "id": "JSsoAEMvC6Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n",
        "print('NMS APPLIED MODEL OUTPUT')\n",
        "#plot_img_bbox(torch_to_pil(img), nms_prediction)"
      ],
      "metadata": {
        "id": "gljyIRp_C_j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SelfDrivingCarDataset(test_dir, 1280, 720, transforms= get_transform(train=True))\n",
        "# pick one image from the test set\n",
        "img, target = test_dataset[10]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "    \n",
        "print('EXPECTED OUTPUT\\n')\n",
        "plot_img_bbox(torch_to_pil(img), target)\n",
        "print('MODEL OUTPUT\\n')\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
        "\n",
        "plot_img_bbox(torch_to_pil(img), nms_prediction)"
      ],
      "metadata": {
        "id": "rgjtJfDqDCph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}